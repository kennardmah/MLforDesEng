{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55b5a3a",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "1. Understand how to use pytorch to write a neural network\n",
    "2. Write a neural network with multiple hidden layers\n",
    "\n",
    "Linear layers are used in this lab, \n",
    "- 3 hidden layers \n",
    "- MNIST database: 28*28 pixel input (= 784 dimensional vector, a 784 1-dimensional tensor)\n",
    "- The output layer can be an integer btw 0 and 9 (= 10 dimensional vector, a 10 1-dimensional tensor)\n",
    "- sigmoid activation function for hidden layers\n",
    "- softmax function for output layer\n",
    "\n",
    "![](img/lab3/network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1673e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "train = datasets.MNIST(\"\", train = True, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train = False, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# DataLoader() wraps an iterable over the given dataset and supports automatic batching, sampling, shuffling and multiprocess data loading\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)\n",
    "# batch_size determine how many samples to pass through network before w and b are updated\n",
    "# reduces memory downloads and increases speed to train\n",
    "# one epoch involves number of samples/batch_size updates to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "207e6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 128 # number of neurons\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, n) # creating a layer of n neurons with 28*28 inputs\n",
    "        self.fc2 = nn.Linear(n, n) # nn.Linear is an affine trasformation\n",
    "        self.fc3 = nn.Linear(n, n)\n",
    "        self.fc4 = nn.Linear(n, n)\n",
    "        self.fc5 = nn.Linear(n, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x)) # 0.103 (64 neurons)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x)) # number of layers\n",
    "        # x = torch.sigmoid(self.fc4(x)) # better accuracy without fourth layer\n",
    "        # x = F.relu(self.fc1(x)) # 0.118 (64 neurons)\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = F.relu(self.fc3(x))\n",
    "        # x = torch.tanh(self.fc1(x)) # 0.411 (64 neurons), 0.47 (128 neurons), 0.353 (200 neurons)\n",
    "        # x = torch.tanh(self.fc2(x))\n",
    "        # x = torch.tanh(self.fc3(x))\n",
    "        # x = torch.tanh(self.fc4(x)) # 0.442 (128 neurons, 4 hidden layers)\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        # computes to softmax function of given input\n",
    "        return F.softmax(x, dim = 1)\n",
    "\n",
    "# create instance of neural network\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0da63",
   "metadata": {},
   "source": [
    "##### Tested with different activation functions, optimisers, and number of layers\n",
    "\n",
    "Best computation was with Sigmoids and Adam. Number of hidden layers, as well as neurons, can affect the accuracy. Increasing the epochs with SGD will be very slow.\n",
    "\n",
    "##### What is ReLu?\n",
    "The ReLu function stands for rectified linear activation function, which is a piecewise linear function that will output the inpur directly if it is positive, other it will output zero.\n",
    "\n",
    "##### Sigmoid VS Relu?\n",
    "Not dataset dependent, and choice of algorithm is guided by the problem, which requires lots of practice to get better at.\n",
    "\n",
    "##### What is Adam gradient descent?\n",
    "Adams is an adaptive method, as opposed to SGD, which uses a combination of momentum and adaptive learning rate to speed up the converge of the optimisation process, and avoid getting stuck. \n",
    "\n",
    "1) Adaptive Learning Rate (adapts learning rate for each weight during training, converge faster and prevents oscillations)\n",
    "2) Momentum Updates (keeps a running average of the gradient updates)\n",
    "\n",
    "- Adam adapts the learning rate for each parameter based on the estimated first and second moments of gradients as opposed to using a fixed learning rate.\n",
    "- Momentum allows the alg to continue moving in the same direction as the gradients, even when they are small or noisy.\n",
    "- Its robustness makes it perform well even with noisy gradients, and when there are many local minima.\n",
    "- Important to note, it is not always the best choice for every problem and requires trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f7f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up optimisation method for backpropagation alg\n",
    "# optimiser = optim.SGD(net.parameters(), lr = 0.001) # SGD (Stoachastic Gradient Descent)\n",
    "optimiser = optim.Adam(net.parameters(), lr = 0.001) # Adam (increases Accuracy drastically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c1be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 3 # number of training epochs\n",
    "\n",
    "# iterate over the training data (trainset)\n",
    "for epoch in range(Epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data # assign input (X) and labels (y)\n",
    "        net.zero_grad() # set gradiant stored to zero to reset gradient value for each iteration\n",
    "        output = net.forward(X.view(-1,28*28)) # transform 2 dimensional tensor (28x28 matrix) input to 1 dimension (784 vector)\n",
    "        loss = F.nll_loss(output, y) # loss function (cross-entropy sicne we are working w classifier)\n",
    "        loss.backward() # compute gradient wrt loss function over each parameter of the network (must set gradient to 0, line 46)\n",
    "        optimiser.step() # update parameters of the network according to the optimisation alg and gradient stored within each variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41073f85",
   "metadata": {},
   "source": [
    "##### Why can't the gradients be automatically zeroed when loss.backward() is called?\n",
    "\n",
    "- net.zero_grad() sets the gradient as 0, as by default, the gradients are accumulated in buffers and not overwritten whenever backwards() is called.\n",
    "- the previous gradient is needed in two cases: 1) when we want to perform gradient descent, as optimiser.step() is called after loss.backward(). 2) We need to accumulate gradient amongst some batches; to do that, we can simply call backward multiple times and optimise once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ce4cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.938\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad(): # network won't update gradient stored in each variable in the test sessions\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1a9cd",
   "metadata": {},
   "source": [
    "##### What is a tensor?\n",
    "A tensor is a data structure used to represent and manipulate complex data in neural networks. 1-dimensional tensors are a vector. They are used to represent input data, model parameters, and intermediate computations in neural networks. Tensors offer several advantages over vectors, it can represent data with any number of dimensions, it is easy to manipulate because they can be reshaped, transposed, sliced, concatenated, making it easier to preprocess and trasform for use in ML models. ML libaries are optimised for tensor operations, enabling faster and more efficient computation. It is also compatable with ML frameworks, which uses them as a common data format.\n",
    "![](img/lab3/tensors.png)\n",
    "\n",
    "##### Result of adding more layers? why does the accuracy go down?\n",
    "\n",
    "The decrease in accuracy suggests that these layers are leading to overfitting, which implies that it is a good fit for the training data, but does not work well with new data, as it learns specific noise and trends that may not be general to all data.\n",
    "\n",
    "Also increase in layers may require much more epochs for it to converge, which takes A LOT of time (could add a stopping criteria rather than running for so many epochs).\n",
    "\n",
    "##### Choosing the number of hidden layers and nodes in a feedforward neural network\n",
    "\n",
    "There are three types of layers: input, hidden and output.\n",
    "\n",
    "Input Layer - the number of neurons comprising the layer is equal to the number of features (columns) in your data, with some adding an additional node for a bias term.\n",
    "\n",
    "Hidden Layer - responsible for transofmring the input data into a form that can be used to make predictions, there can be one or more. \n",
    "\n",
    "Output Layer - one output layer, number of nodes depends on the output that the model is designed to produce."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
