{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b0077a",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "- Write a convolutional neural network for the MNIST Database\n",
    "- Unlike the previous lab, we are going to make use of convolutional networks instead of linear to classify the inputs\n",
    "\n",
    "![](img/lab4/convolutionalnetwork.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b128759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "train = datasets.MNIST(\"\", train = True, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train = False, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)\n",
    "# Batchsize is a parameter that determines the number of samples in each batch before updating the optimizers (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3365d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2) # first two represents dimension, and 5 represents dimension of convolution kernel\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2) # convolution kernel is 5x5 in this example, why is padding = 2?\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2)) # 2x2 patch\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x): # take an input tensor, x, and produce an output tensor\n",
    "        x = self.convs(x) # applies the defined self.convs modules, reduces spatial dimensions\n",
    "        x = x.view(-1, 64*7*7) # reshapes output tensor into a 1-D tensor (-1) the output has 64 channels and the height and width is reduced to 7 (due to pooling)\n",
    "        x = F.relu(self.fc1(x)) # applies linear layer to the flattened output tensor followed by ReLU activation, adding non-linear element to the model\n",
    "        x = self.fc2(x) # applies linear layer\n",
    "\n",
    "        # computes to softmax function of given input\n",
    "        return F.softmax(x, dim = 1)\n",
    "\n",
    "# create instance of neural network\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7779137",
   "metadata": {},
   "source": [
    "self.conv: 1 (input channel, as it is grayscale), 32 (output channel, number of filters to be used), 5 (kernel size, size of the convolutional filter in this case 5), padding = 2 (amount of padding added to the input image to ensure that the output feature map has the same spatial dimensions, 2 pixels is added to each sides of the image)\n",
    "\n",
    "##### what is the padding for?\n",
    "padding is used to preserve the spatial resolution of the input image and prevent information loss at the edges of the image during convolution\n",
    "\n",
    "##### what is the point of pooling?\n",
    "pooling is used to reduce the spatial dimensions while retaining the most important information. This reduces the parameters in the network and prevents overfitting. Max pooling takes the maximum value of a local neighborhood pixels (in this case a 2x2 patch), resulting ina smaller output feature map. Overall, pooling helps make the network computationally more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1846042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up optimisation method for backpropagation alg\n",
    "# optimiser = optim.SGD(net.parameters(), lr = 0.001) # SGD (Stoachastic Gradient Descent)\n",
    "optimiser = optim.Adam(net.parameters(), lr = 0.001) # Adam (increases Accuracy drastically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b860e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.972\n"
     ]
    }
   ],
   "source": [
    "Epochs = 3 # number of training epochs\n",
    "\n",
    "# iterate over the training data (trainset)\n",
    "for epoch in range(Epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data # assign input (X) and labels (y)\n",
    "        net.zero_grad() # set gradiant stored to zero to reset gradient value for each iteration\n",
    "        output = net.forward(X) # change as the network is given a matrix instead of a vector for input\n",
    "        loss = F.nll_loss(output, y) # loss function (cross-entropy sicne we are working w classifier)\n",
    "        loss.backward() # compute gradient wrt loss function over each parameter of the network (must set gradient to 0, line 48)\n",
    "        optimiser.step() # update parameters of the network according to the optimisation alg and gradient stored within each variable\n",
    "\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X)\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de8ec95",
   "metadata": {},
   "source": [
    "### Possible Questions\n",
    "##### Why does it take significantly more time?\n",
    "It takes longer time to compute an epoch, likely because they involve additional computational steps such as convolutions and pooling, which can be computationally expensive compared to the simpler matrix multiplications in ANNs.\n",
    "##### Changing the number of neurons?\n",
    "##### Changing the optimiser and activation functions?\n",
    "##### Modifying the pooling layer?\n",
    "##### Modifying the dimensions of the convolutional kernel (and padding)?\n",
    "\n",
    "###### Is it reasonable to expect to reach 100% classification accuracy?\n",
    "\n",
    "Achieving 100% accuracy on MNIST dataset is possible, given that it is a relatively simple task. However, it is still possible to overfit, especially if the model is too complex, and therefore, the training set would not be representative of the general dataset or new data. Essentially it would be remembering the patterns of the training set if overfit rather than the actual dataset.\n",
    "\n",
    "Achieving high accuracy on the MNIST is not a sufficient condition for the model to be considered good, and the model should be evaluated on its ability to generalise to new data, and its performance should be compared to other models on the same tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
